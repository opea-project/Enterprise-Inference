# Inference API Configuration
# INFERENCE_API_ENDPOINT: URL to your inference service (without /v1 suffix)
#   - For GenAI Gateway: https://genai-gateway.example.com
#   - For APISIX Gateway: https://apisix-gateway.example.com/Llama-3.1-8B-Instruct
#     Note: APISIX Gateway requires the model name in the URL path
#
# INFERENCE_API_TOKEN: Authentication token/API key for the inference service
#   - For GenAI Gateway: Your GenAI Gateway API key
#   - For APISIX Gateway: Your APISIX authentication token
INFERENCE_API_ENDPOINT=https://api.example.com
INFERENCE_API_TOKEN=your-pre-generated-token-here

# Model Configuration
# IMPORTANT: Use the full model names as they appear in your inference service
# Check available models: curl https://your-api-endpoint.com/v1/models -H "Authorization: Bearer your-token"
INFERENCE_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct

# LLM Configuration
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000

# Local URL Endpoint (only needed for non-public domains)
# If using a local domain like api.example.com mapped to localhost:
#   Set this to: api.example.com (domain without https://)
# If using a public domain, set any placeholder value like: not-needed
LOCAL_URL_ENDPOINT=not-needed

# Service Configuration
SERVICE_PORT=8000
LOG_LEVEL=INFO

# File Upload Limits
MAX_FILE_SIZE=524288000
MAX_PDF_SIZE=52428800
MAX_PDF_PAGES=100
